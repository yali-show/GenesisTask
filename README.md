## Коротко про задачу:

Є API з 4 методами які повертають різні дані в різних форматах

(/installs, /events, /orders, /costs)

Користуючись Google Cloud Platform, налаштувати щоденне вивантаження даних із API.

На основі цих даних, зібрати Data Mart, для побудови Dashboard оцінки ефективності маркетингу (CPI, Revenue, ROAS)

___
___
## Інструменти під час розробки

Використовую парадигму проагрмування ООП для зручнішого розуміння коду та задачі.

Для зберігання DataMart та роботи з гугл клаудом ==> google.cloud  storage, bigquery.

Для роботи з API використовував бібліотеку requests --> зручніше як на мене чим urllib.

Для обробки csv та деяких responses викотристовую pandas (DataFrame та його методи).

Для обробки контексту методу /orders який повертав apache-parquet-pandas знадобилось спочатку декодити (в моєму випадку пакет іо), потім за допомогою pyarrow.parquet імпортувати в pandas DataFrame.

DataMart буде зберігатись в google cloud storage у вигляді csv:

### DataMart мого зразку 
***(з рандоминими даними)***



| date       | cpi     | revenue | roas    |
|------------|---------|---------|---------|
| 2023-12-10 | 123.000 | 111.000 | 334.000 |
| 2023-12-11 | 234.000 | 222.000 | 456.000 |
| 2023-12-12 | 456.000 | 333.000 | 334.000 |

---

---

### GCP логіка для DataMart

1) Якщо файл DataMart не існує сетапимо данні інакше оновлюємо файл та надсилаємо на storage.


### GCP інструменти

 1. Pub/sub для автоматичного запуску коду на подію.
2. Scheduler таймер який привʼязаний до Pub/sub. Запускає event для Pub/sub.
 3. Fucntion підіймає машину виконює код та вимикається (для меншої затратності грошей).
Привʼязаний до Pub/sub.
4. Storage для зберігання даних (коду для функції та аутпутів) у моєму випадку це був архів з кодом та csv файл DataMart
 ---